{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Urls\n",
    "# url = 'https://medium.com/@getgoing.ca/the-ultimate-guide-to-choosing-the-perfect-car-19747eec95e'\n",
    "url = 'https://datascience.codata.org/'\n",
    "url ='https://www.vogue.com/fashion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to scrape text data from a given URL\n",
    "def scrape(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    scraped_data = []\n",
    "    html_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']\n",
    "    for tag in html_tags:\n",
    "        elements = soup.find_all(tag)\n",
    "        texts = [element.text.strip() for element in elements if len(element.text.strip()) > 30]\n",
    "        scraped_data.extend(texts)\n",
    "    scraped_data = pd.Series(scraped_data)\n",
    "    return scraped_data\n",
    "\n",
    "# Function to clean the scraped text data\n",
    "def text_cleaner(text):\n",
    "    serie_joined = ' '.join(text)\n",
    "    doc = nlp(serie_joined)\n",
    "    clean_words = []\n",
    "    for each in doc:\n",
    "        if each.is_digit or each.like_url or each.like_email or each.is_punct:\n",
    "            continue\n",
    "        else:\n",
    "            clean_words.append(each)\n",
    "    return clean_words\n",
    "\n",
    "# Wrapper function to scrape and clean text data from a given URL\n",
    "def scraper(url):\n",
    "    scr = scrape(url)\n",
    "    clt = text_cleaner(scr)\n",
    "    words = [each.text for each in clt]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to process multiple URLs\n",
    "def scraper_results(url_inputs):\n",
    "    results = []\n",
    "    for url in url_inputs:\n",
    "        result = scraper(url)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "# Function to embed text using the model\n",
    "def embed(text):\n",
    "    embedding = model.encode(text)\n",
    "    return embedding\n",
    "\n",
    "# List of target interests\n",
    "targets = [\n",
    "    \"Fashion\", \"Pets\", \"Cooking\", \"Fitness\", \"Movies\", \"Gaming\", \"Travel\",\n",
    "    \"Cars and automobiles\", \"Outdoor activities\", \"Books\", \"Finance and investments\",\n",
    "    \"Business and entrepreneurship\", \"Photography\", \"Art\", \"Social causes and activism\",\n",
    "    \"Health and wellness\", \"Gardening\", \"Technology\", \"Education and learning\",\n",
    "    \"Sports\", \"Nature\", \"History\", \"Parenting and family\", \"Music\",\n",
    "    \"Food and dining\", \"DIY and crafts\", \"Beauty\", \"Science\", \"Politics\"\n",
    "]\n",
    "\n",
    "# Create DataFrame of interests and their embeddings\n",
    "interests = pd.DataFrame({\"interest\": targets})\n",
    "interests[\"embedding\"] = interests.interest.apply(embed)\n",
    "\n",
    "# Scrape and embed text data from the given URL\n",
    "text_embedding = embed(scraper(url))\n",
    "\n",
    "# Calculate cosine similarity between the text embedding and interest embeddings\n",
    "similarities = interests['embedding'].apply(lambda x: cosine_similarity([x], [text_embedding])[0][0])\n",
    "\n",
    "# Find the best fit interest\n",
    "best_fit_index = similarities.idxmax()\n",
    "best_fit_interest = interests.loc[best_fit_index, 'interest']\n",
    "\n",
    "# Print the best fit interest\n",
    "print(f\"The best fit interest is: {best_fit_interest}\")\n",
    "\n",
    "# Load additional datasets\n",
    "interests_df = pd.read_csv('/Users/poloniki/code/marta/example_project/data_v2.csv')\n",
    "social_media_data =  ds/SocialMediaUsersDataset.csv')\n",
    "clusters = pd.read_csv('/Users/poloniki/code/marta/example_project/data_v2.csv')\n",
    "\n",
    "# Filter data by best fit interest\n",
    "filter_by_interest = clusters.loc[clusters.Interests.str.contains(best_fit_interest)]\n",
    "\n",
    "# Find the cluster with the most occurrences of the best fit interest\n",
    "best_cluster = filter_by_interest.cluster.value_counts().index[0]\n",
    "\n",
    "# Calculate the average age of the best cluster\n",
    "avg_age_of_cluster = clusters.loc[clusters.cluster == best_cluster].Age.mean()\n",
    "\n",
    "# Get data of the best cluster\n",
    "best_cluster_df = clusters.loc[clusters.cluster == best_cluster]\n",
    "\n",
    "# Evaluate the interests in the best cluster\n",
    "best_cluster_df['Interests_eval'] = best_cluster_df.Interests.apply(lambda x: list(eval(x) if ',' in x else [x]))\n",
    "\n",
    "# Find the top 5 other interests in the best cluster\n",
    "top_5_other_interests = best_cluster_df.explode('Interests_eval').Interests_eval.value_counts()[1:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion 55 Interests_eval\n",
      "Outdoor activities    1907\n",
      "Movies                1906\n",
      "DIY and crafts        1884\n",
      "Fitness               1874\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Output the best fit interest, average age, and top 5 other interests\n",
    "print(best_fit_interest, int(avg_age_of_cluster), top_5_other_interests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Explanation\n",
    "\n",
    "## Data Scraping and Cleaning\n",
    "\n",
    "1. **Scrape Data from URL:**\n",
    "   - The `scrape(url)` function collects text data from the given URL. It looks for text inside specific HTML tags like `h1`, `h2`, `p`, etc.\n",
    "   \n",
    "2. **Clean the Scraped Text:**\n",
    "   - The `text_cleaner(text)` function cleans the collected text by removing numbers, URLs, emails, and punctuation marks. This helps in keeping only the meaningful words.\n",
    "\n",
    "3. **Combine Scraping and Cleaning:**\n",
    "   - The `scraper(url)` function uses both the `scrape(url)` and `text_cleaner(text)` functions to get clean text data from the URL.\n",
    "   - The `scraper_results(url_inputs)` function processes multiple URLs by applying the `scraper(url)` function to each one.\n",
    "\n",
    "## Embedding and Similarity Calculation\n",
    "\n",
    "### What is an Embedding?\n",
    "- An embedding is a way to convert text into numbers so that we can use it with machine learning models. Here, we use the `SentenceTransformer` model to create embeddings for our text.\n",
    "\n",
    "1. **Load Sentence Transformer Model:**\n",
    "   - The `SentenceTransformer` model is used to convert the text data into embeddings (numerical representations).\n",
    "\n",
    "2. **Create Embeddings:**\n",
    "   - The `embed(text)` function generates an embedding for the given text using the model.\n",
    "\n",
    "3. **List of Target Interests:**\n",
    "   - We have a list of interests (like Fashion, Pets, Cooking, etc.) and we create embeddings for each interest.\n",
    "\n",
    "4. **Calculate Similarity:**\n",
    "   - We calculate the similarity between the text embedding (from the URL) and the interest embeddings. This helps us find the interest that best matches the text data.\n",
    "\n",
    "## Cluster Analysis\n",
    "\n",
    "1. **Load Additional Data:**\n",
    "   - We load datasets that Wei created, which have user profiles and their retrospective clusters.\n",
    "\n",
    "2. **Filter Data by Best Fit Interest:**\n",
    "   - We filter the dataset to find users who have an interest that matches our best fit interest.\n",
    "\n",
    "3. **Identify the Best Cluster:**\n",
    "   - We find the cluster with the most occurrences of the best fit interest.\n",
    "\n",
    "4. **Calculate Average Age:**\n",
    "   - We calculate the average age of users in the identified cluster.\n",
    "\n",
    "5. **Find Top 5 Other Interests:**\n",
    "   - We look at the top 5 other interests that users in the best cluster have.\n",
    "\n",
    "## Output\n",
    "\n",
    "- The code outputs the best fit interest, the average age of users in the best cluster, and the top 5 other interests in that cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
